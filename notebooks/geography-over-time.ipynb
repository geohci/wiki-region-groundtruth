{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region coverage on Wikipedia over time\n",
    "Take current groundtruth data about the regions associated with each Wikidata item and map that to articles on Wikipedia and when they were created to generate data on the representation of different regions by wiki by month from the start of Wikipedia to today.\n",
    "\n",
    "Caveats:\n",
    "* I ignore articles that were deleted. For most wikis, this should have minimal impact. For others like Swedish Wikipedia that has deleted a large number of articles that are geographic, this might impact the results in a noticeable way. It would be a lot more work to track deleted articles too though and still imperfect and isn't clearly a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "import wmfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/25 19:09:13 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12001. Attempting port 12002.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12002. Attempting port 12003.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12003. Attempting port 12004.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12004. Attempting port 12005.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12005. Attempting port 12006.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12006. Attempting port 12007.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12007. Attempting port 12008.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12008. Attempting port 12009.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12009. Attempting port 12010.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12010. Attempting port 12011.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12011. Attempting port 12012.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12012. Attempting port 12013.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12013. Attempting port 12014.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12014. Attempting port 12015.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12015. Attempting port 12016.\n",
      "23/01/25 19:09:13 WARN Utils: Service 'sparkDriver' could not bind on port 12016. Attempting port 12017.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "23/01/25 19:09:14 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13001. Attempting port 13002.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13002. Attempting port 13003.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13003. Attempting port 13004.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13004. Attempting port 13005.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13005. Attempting port 13006.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13006. Attempting port 13007.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13007. Attempting port 13008.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13008. Attempting port 13009.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13009. Attempting port 13010.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13010. Attempting port 13011.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13011. Attempting port 13012.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13012. Attempting port 13013.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13013. Attempting port 13014.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13014. Attempting port 13015.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13015. Attempting port 13016.\n",
      "23/01/25 19:09:22 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13016. Attempting port 13017.\n",
      "23/01/25 19:09:22 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "spark = wmfdata.spark.create_session(app_name='pyspark regular; regions; isaacj',\n",
    "                                  type='yarn-regular', # local, yarn-regular, yarn-large\n",
    "                                  )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_gt_table = 'isaacj.qid_to_country'\n",
    "initial_gt_snapshot = '2022-01-03'\n",
    "wikidata_snapshot = '2023-01-02'\n",
    "mediawiki_snapshot = '2022-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+----------+\n",
      "|qid      |property|country|snapshot  |\n",
      "+---------+--------+-------+----------+\n",
      "|Q34757124|P625    |Tuvalu |2022-01-03|\n",
      "|Q7068891 |P625    |Tuvalu |2022-01-03|\n",
      "|Q16980370|P625    |Tuvalu |2022-01-03|\n",
      "|Q49314729|P625    |Tuvalu |2022-01-03|\n",
      "|Q3394461 |P625    |Tuvalu |2022-01-03|\n",
      "|Q11762732|P625    |Tuvalu |2022-01-03|\n",
      "|Q632495  |P625    |Tuvalu |2022-01-03|\n",
      "|Q49106845|P625    |Tuvalu |2022-01-03|\n",
      "|Q34967204|P625    |Tuvalu |2022-01-03|\n",
      "|Q6918786 |P625    |Tuvalu |2022-01-03|\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f'SELECT * FROM {initial_gt_table} where snapshot = \"{initial_gt_snapshot}\" LIMIT 10').show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH wikis AS (\n",
      "    SELECT DISTINCT\n",
      "      database_code\n",
      "    FROM canonical_data.wikis\n",
      "    WHERE\n",
      "      database_group = 'wikipedia'\n",
      "      AND status = 'open'\n",
      "      AND visibility = 'public'\n",
      "      AND editability = 'public'\n",
      "),\n",
      "regions AS (\n",
      "    SELECT DISTINCT\n",
      "      qid,\n",
      "      country\n",
      "    FROM isaacj.qid_to_country\n",
      "    WHERE\n",
      "      snapshot = '2022-01-03'\n",
      "),\n",
      "page_creation_dates AS (\n",
      "    SELECT DISTINCT\n",
      "      wiki_db,\n",
      "      page_id,\n",
      "      CONCAT(YEAR(page_creation_timestamp), \"-\", LPAD(MONTH(page_creation_timestamp), 2, \"0\")) AS creation_month\n",
      "    FROM wmf.mediawiki_page_history mph\n",
      "    INNER JOIN wikis w\n",
      "      ON (mph.wiki_db = w.database_code)\n",
      "    WHERE\n",
      "      snapshot = '2022-12'\n",
      "      AND page_namespace = 0\n",
      "      AND NOT page_is_deleted\n",
      "      AND NOT page_is_redirect\n",
      "      AND end_timestamp IS NULL\n",
      "      AND page_creation_timestamp IS NOT NULL\n",
      "),\n",
      "pageid_to_qid AS (\n",
      "    SELECT\n",
      "      wiki_db,\n",
      "      page_id,\n",
      "      item_id\n",
      "    FROM wmf.wikidata_item_page_link wipl\n",
      "    INNER JOIN wikis w\n",
      "      ON (wipl.wiki_db = w.database_code)\n",
      "    WHERE\n",
      "      snapshot = '2023-01-02'\n",
      "      AND page_namespace = 0\n",
      "),\n",
      "article_country_creation AS (\n",
      "    SELECT\n",
      "      pcd.wiki_db,\n",
      "      creation_month,\n",
      "      COALESCE(country, 'non-geo') AS geo,\n",
      "      COUNT(1) AS num_articles\n",
      "    FROM page_creation_dates pcd\n",
      "    INNER JOIN pageid_to_qid ptq\n",
      "      ON (pcd.wiki_db = ptq.wiki_db\n",
      "          AND pcd.page_id = ptq.page_id)\n",
      "    LEFT JOIN regions r\n",
      "      ON (ptq.item_id = r.qid)\n",
      "    GROUP BY\n",
      "      pcd.wiki_db,\n",
      "      creation_month,\n",
      "      geo\n",
      ")\n",
      "SELECT\n",
      "  wiki_db,\n",
      "  creation_month,\n",
      "  geo,\n",
      "  num_articles AS num_created,\n",
      "  SUM(num_articles) OVER w as total_articles\n",
      "FROM article_country_creation\n",
      "WINDOW w AS (PARTITION BY wiki_db, geo ORDER BY creation_month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
      "ORDER BY\n",
      "  wiki_db ASC,\n",
      "  creation_month ASC,\n",
      "  geo ASC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "# CTDs:\n",
    "#  * wikis: limit to just Wikipedia wikis\n",
    "#  * regions: get mapping of QID to country (a QID can be associated with 0+ countries)\n",
    "#  * page_creation_dates: get creation months for existing articles in each wiki\n",
    "#  * pageid_to_qid: mapping of page ID to Wikidata ID for joining in country data\n",
    "#  * article_country_creation: join all data for counts of articles created per wiki/region/month\n",
    "#  * SELECT...: add in cumulative sum of articles that existed for each geo by month\n",
    "\n",
    "query = f\"\"\"\n",
    "WITH wikis AS (\n",
    "    SELECT DISTINCT\n",
    "      database_code\n",
    "    FROM canonical_data.wikis\n",
    "    WHERE\n",
    "      database_group = 'wikipedia'\n",
    "      AND status = 'open'\n",
    "      AND visibility = 'public'\n",
    "      AND editability = 'public'\n",
    "),\n",
    "regions AS (\n",
    "    SELECT DISTINCT\n",
    "      qid,\n",
    "      country\n",
    "    FROM {initial_gt_table}\n",
    "    WHERE\n",
    "      snapshot = '{initial_gt_snapshot}'\n",
    "),\n",
    "page_creation_dates AS (\n",
    "    SELECT DISTINCT\n",
    "      wiki_db,\n",
    "      page_id,\n",
    "      CONCAT(YEAR(page_creation_timestamp), \"-\", LPAD(MONTH(page_creation_timestamp), 2, \"0\")) AS creation_month\n",
    "    FROM wmf.mediawiki_page_history mph\n",
    "    INNER JOIN wikis w\n",
    "      ON (mph.wiki_db = w.database_code)\n",
    "    WHERE\n",
    "      snapshot = '{mediawiki_snapshot}'\n",
    "      AND page_namespace = 0\n",
    "      AND NOT page_is_deleted\n",
    "      AND NOT page_is_redirect\n",
    "      AND end_timestamp IS NULL\n",
    "      AND page_creation_timestamp IS NOT NULL\n",
    "),\n",
    "pageid_to_qid AS (\n",
    "    SELECT\n",
    "      wiki_db,\n",
    "      page_id,\n",
    "      item_id\n",
    "    FROM wmf.wikidata_item_page_link wipl\n",
    "    INNER JOIN wikis w\n",
    "      ON (wipl.wiki_db = w.database_code)\n",
    "    WHERE\n",
    "      snapshot = '{wikidata_snapshot}'\n",
    "      AND page_namespace = 0\n",
    "),\n",
    "article_country_creation AS (\n",
    "    SELECT\n",
    "      pcd.wiki_db,\n",
    "      creation_month,\n",
    "      COALESCE(country, 'non-geo') AS geo,\n",
    "      COUNT(1) AS num_articles\n",
    "    FROM page_creation_dates pcd\n",
    "    INNER JOIN pageid_to_qid ptq\n",
    "      ON (pcd.wiki_db = ptq.wiki_db\n",
    "          AND pcd.page_id = ptq.page_id)\n",
    "    LEFT JOIN regions r\n",
    "      ON (ptq.item_id = r.qid)\n",
    "    GROUP BY\n",
    "      pcd.wiki_db,\n",
    "      creation_month,\n",
    "      geo\n",
    ")\n",
    "SELECT\n",
    "  wiki_db,\n",
    "  creation_month,\n",
    "  geo,\n",
    "  num_articles AS num_created,\n",
    "  SUM(num_articles) OVER w as total_articles\n",
    "FROM article_country_creation\n",
    "WINDOW w AS (PARTITION BY wiki_db, geo ORDER BY creation_month ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n",
    "ORDER BY\n",
    "  wiki_db ASC,\n",
    "  geo ASC,\n",
    "  creation_month ASC,\n",
    "\"\"\"\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)\n",
    "    result.coalesce(1).write.csv(path=\"/user/isaacj/wiki-month-geo-counts\", compression=\"gzip\", header=True, sep=\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -copyToLocal wiki-month-geo-counts/part-00000-35714848-f22e-4921-814a-d3d5e56e0dfa-c000.csv.gz wiki-month-geo-counts.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_db\tcreation_month\tgeo\tnum_created\ttotal_articles\n",
      "abwiki\t2012-01\tAfghanistan\t1\t1\n",
      "abwiki\t2017-10\tAfghanistan\t5\t6\n",
      "abwiki\t2019-07\tAfghanistan\t2\t8\n",
      "abwiki\t2021-10\tAfghanistan\t1\t9\n",
      "abwiki\t2010-06\tAlbania\t2\t2\n",
      "abwiki\t2010-09\tAlbania\t1\t3\n",
      "abwiki\t2013-11\tAlbania\t1\t4\n",
      "abwiki\t2016-08\tAlbania\t1\t5\n",
      "abwiki\t2017-10\tAlbania\t1\t6\n"
     ]
    }
   ],
   "source": [
    "!zless wiki-month-geo-counts.tsv.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing months\n",
    "# e.g., if 1 article created for a country/wiki in 2001-05\n",
    "# and then the next created in 2001-07, then 2001-06 should be added\n",
    "# with 0 created articles but 1 existing article.\n",
    "\n",
    "all_months = []\n",
    "for y in range(2001, 2023):\n",
    "    for m in range(1, 13):\n",
    "        all_months.append(f'{y}-{str(m).rjust(2, \"0\")}')\n",
    "all_months_map = {m:i for i,m in enumerate(all_months)}\n",
    "\n",
    "prev_wiki = None\n",
    "prev_geo = None\n",
    "prev_month_idx = None\n",
    "prev_total_articles = 0\n",
    "with gzip.open('wiki-month-geo-counts.tsv.gz', 'rt') as fin:\n",
    "    with open('wiki-month-geo-counts-imputed.tsv', 'w') as fout:\n",
    "        fout.write(next(fin))  # header\n",
    "        for line in fin:\n",
    "            wiki_db, creation_month, geo, num_created, total_articles = line.strip().split('\\t')\n",
    "            if creation_month == '2023-01':\n",
    "                continue\n",
    "            if wiki_db == prev_wiki and prev_geo == geo:\n",
    "                curr_month_idx = all_months_map[creation_month]\n",
    "                for midx in range(prev_month_idx + 1, curr_month_idx):\n",
    "                    fout.write('\\t'.join((wiki_db, all_months[midx], geo, str(0), str(prev_total_articles))) + '\\n')\n",
    "                fout.write(line)\n",
    "                prev_month_idx = curr_month_idx\n",
    "                prev_total_articles = total_articles\n",
    "            else:\n",
    "                fout.write(line)\n",
    "                prev_wiki = wiki_db\n",
    "                prev_geo = geo\n",
    "                prev_month_idx = all_months_map[creation_month]\n",
    "                prev_total_articles = total_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_db\tcreation_month\tgeo\tnum_created\ttotal_articles\n",
      "abwiki\t2012-01\tAfghanistan\t1\t1\n",
      "abwiki\t2017-10\tAfghanistan\t5\t6\n",
      "abwiki\t2019-07\tAfghanistan\t2\t8\n",
      "abwiki\t2021-10\tAfghanistan\t1\t9\n",
      "abwiki\t2010-06\tAlbania\t2\t2\n",
      "abwiki\t2010-09\tAlbania\t1\t3\n",
      "abwiki\t2013-11\tAlbania\t1\t4\n",
      "abwiki\t2016-08\tAlbania\t1\t5\n",
      "abwiki\t2017-10\tAlbania\t1\t6\n"
     ]
    }
   ],
   "source": [
    "!zless wiki-month-geo-counts.tsv.gz | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_db\tcreation_month\tgeo\tnum_created\ttotal_articles\n",
      "abwiki\t2012-01\tAfghanistan\t1\t1\n",
      "abwiki\t2012-02\tAfghanistan\t0\t1\n",
      "abwiki\t2012-03\tAfghanistan\t0\t1\n",
      "abwiki\t2012-04\tAfghanistan\t0\t1\n",
      "abwiki\t2012-05\tAfghanistan\t0\t1\n",
      "abwiki\t2012-06\tAfghanistan\t0\t1\n",
      "abwiki\t2012-07\tAfghanistan\t0\t1\n",
      "abwiki\t2012-08\tAfghanistan\t0\t1\n",
      "abwiki\t2012-09\tAfghanistan\t0\t1\n"
     ]
    }
   ],
   "source": [
    "!head wiki-month-geo-counts-imputed.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
