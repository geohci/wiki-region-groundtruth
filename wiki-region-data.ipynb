{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/isaacj/.conda/envs/2020-12-01T16.32.19_isaacj/bin/python\n"
     ]
    }
   ],
   "source": [
    "# using custom conda environment so I can include shapely for the coordinate -> country look-up\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge shapely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip up conda environment to ship to workers\n",
    "# see: https://wikitech.wikimedia.org/wiki/Analytics/Systems/Jupyter#Launching_as_SparkSession_in_a_Python_Notebook\n",
    "!cd '/home/isaacj/.conda/envs/2020-12-01T16.32.19_isaacj'; zip -qur ~/spark_venv.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 isaacj wikidev 747M Dec 22 15:08 spark_venv.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -lht spark_venv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from shapely.geometry import shape, Point\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1008.eqiad.wmnet:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark notebook (isaacj -- spatial)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe8af213850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup Spark environment\n",
    "# this is the equivalent of a \"large\" YARN config\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--archives spark_venv.zip#venv pyspark-shell'\n",
    "os.environ['PYSPARK_PYTHON'] = 'venv/bin/python'\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('Pyspark notebook (isaacj -- spatial)')\n",
    "    .master('yarn')\n",
    "    .config(\n",
    "        'spark.driver.extraJavaOptions',\n",
    "        ' '.join('-D{}={}'.format(k, v) for k, v in {\n",
    "            'http.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'http.proxyPort': '8080',\n",
    "            'https.proxyHost': 'webproxy.eqiad.wmnet',\n",
    "            'https.proxyPort': '8080',\n",
    "        }.items()))\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config('spark.dynamicAllocation.maxExecutors', 128)\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 512)\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in supporting data files from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "region_geoms = ('ne_10m_admin_0_map_units.geojson',\n",
    "                'https://github.com/geohci/wiki-region-groundtruth/raw/main/resources/ne_10m_admin_0_map_units.geojson')\n",
    "properties_tsv = ('country_properties.tsv',\n",
    "                  'https://github.com/geohci/wiki-region-groundtruth/raw/main/resources/country_properties.tsv')\n",
    "aggregation_tsv = ('country_aggregation.tsv',\n",
    "                   'https://github.com/geohci/wiki-region-groundtruth/raw/main/resources/country_aggregation.tsv')\n",
    "region_tsv = ('base_regions_qids.tsv',\n",
    "              'https://github.com/geohci/wiki-region-groundtruth/raw/main/resources/base_regions_qids.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 25M\n",
      "-rw-r--r-- 1 isaacj wikidev 4.6K Dec 23 15:06 base_regions_qids.tsv\n",
      "-rw-r--r-- 1 isaacj wikidev 1.8K Dec 23 15:06 country_aggregation.tsv\n",
      "-rw-r--r-- 1 isaacj wikidev  265 Dec 23 15:06 country_properties.tsv\n",
      "-rw-r--r-- 1 isaacj wikidev  25M Dec 23 15:06 ne_10m_admin_0_map_units.geojson\n"
     ]
    }
   ],
   "source": [
    "!rm -R {data_dir}\n",
    "!mkdir -p {data_dir}\n",
    "# have to string commands together or `cd` doesn't apply to later commands\n",
    "!cd {data_dir}; wget -q {region_geoms[1]} {properties_tsv[1]} {aggregation_tsv[1]} {region_tsv[1]}; ls -lht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_properties(properties_tsv):\n",
    "    \"\"\"List of properties used for directly linking Wikidata items to regions.\n",
    "\n",
    "    e.g., P19: place of birth\n",
    "    These are compiled based on knowledge of Wikidata and Marc Miquel's excellent work:\n",
    "    https://github.com/marcmiquel/WDO/blob/e482a2df2b41d389945f3b82179b8b7ca338b8d5/src_data/wikipedia_diversity.py\n",
    "    \"\"\"\n",
    "    expected_header = ['Property', 'Label']\n",
    "    region_properties = []\n",
    "    with open(properties_tsv, 'r') as fin:\n",
    "        tsvreader = csv.reader(fin, delimiter='\\t')\n",
    "        assert next(tsvreader) == expected_header\n",
    "        for line in tsvreader:\n",
    "            property = line[0]\n",
    "            label = line[1]\n",
    "            region_properties.append((property, label))\n",
    "    return region_properties\n",
    "\n",
    "def get_aggregation_logic(aggregates_tsv):\n",
    "    \"\"\"Mapping of QIDs -> regions not directly associated with them.\n",
    "\n",
    "    e.g., Sahrawi Arab Democratic Republic (Q40362) -> Western Sahara (Q6250)\n",
    "    \"\"\"\n",
    "    expected_header = ['Aggregation', 'From', 'QID To', 'QID From']\n",
    "    aggregation = {}\n",
    "    with open(aggregates_tsv, 'r') as fin:\n",
    "        tsvreader = csv.reader(fin, delimiter='\\t')\n",
    "        assert next(tsvreader) == expected_header\n",
    "        for line in tsvreader:\n",
    "            try:\n",
    "                qid_to = line[2]\n",
    "                qid_from = line[3]\n",
    "            except Exception:\n",
    "                print(\"Skipped:\", line)\n",
    "            if qid_from:\n",
    "                aggregation[qid_from] = qid_to\n",
    "    return aggregation\n",
    "\n",
    "def get_region_data(region_qids_tsv, region_geoms_geojson, aggregation_tsv):\n",
    "    # load in canonical mapping of QID -> region name for labeling\n",
    "    qid_to_region = {}\n",
    "    with open(region_qids_tsv, 'r') as fin:\n",
    "        tsvreader = csv.reader(fin, delimiter='\\t')\n",
    "        assert next(tsvreader) == ['Region', 'QID']\n",
    "        for line in tsvreader:\n",
    "            region = line[0]\n",
    "            qid = line[1]\n",
    "            qid_to_region[qid] = region\n",
    "    print(\"\\nLoaded {0} QID-region pairs for matching against Wikidata -- e.g., Q31: {1}\".format(\n",
    "        len(qid_to_region), qid_to_region['Q31']))\n",
    "    # load in additional QIDs that should be mapped to a more canonical region name\n",
    "    aggregation = get_aggregation_logic(aggregation_tsv)\n",
    "    for qid_from in aggregation:\n",
    "        qid_to = aggregation[qid_from]\n",
    "        if qid_to in qid_to_region:\n",
    "            qid_to_region[qid_from] = qid_to_region[qid_to]  \n",
    "        else:\n",
    "            print(\"-- Skipping aggregation for {0} to {1}\".format(qid_from, qid_to))\n",
    "    print(\"Now {0} QID-region pairs after adding aggregations -- e.g., Q40362: {1}\".format(\n",
    "        len(qid_to_region), qid_to_region['Q40362']))\n",
    "\n",
    "    # load in geometries for the regions identified via Wikidata\n",
    "    with open(region_geoms_geojson, 'r') as fin:\n",
    "        regions = json.load(fin)['features']\n",
    "    region_shapes = {}\n",
    "    skipped = []\n",
    "    for c in regions:\n",
    "        qid = c['properties']['WIKIDATAID']\n",
    "        if qid in qid_to_region:\n",
    "            region_shapes[qid] = shape(c['geometry'])\n",
    "        else:\n",
    "            skipped.append('{0} ({1})'.format(c['properties']['NAME'], qid))\n",
    "    print(\"\\nLoaded {0} region geometries. Skipped {1}: {2}\".format(\n",
    "        len(region_shapes), len(skipped), skipped))\n",
    "    \n",
    "    # check alignment between QID list and region geometries\n",
    "    in_common = 0\n",
    "    for qid in qid_to_region:\n",
    "        if qid in region_shapes:\n",
    "            in_common += 1\n",
    "        else:\n",
    "            alt_found = False\n",
    "            for qid_alt in qid_to_region:\n",
    "                if qid != qid_alt and qid_to_region[qid] == qid_to_region[qid_alt]:\n",
    "                    alt_found = True\n",
    "            if not alt_found:\n",
    "                print('Prop-only: {0} ({1})'.format(qid_to_region[qid], qid))\n",
    "    print(\"{0} QIDs in common between prop-values and geometries.\".format(in_common))\n",
    "    return region_shapes, qid_to_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 260 QID-region pairs for matching against Wikidata -- e.g., Q31: Belgium\n",
      "Now 295 QID-region pairs after adding aggregations -- e.g., Q40362: Western Sahara\n",
      "\n",
      "Loaded 280 region geometries. Skipped 15: ['Dhekelia (Q9206745)', 'UNDOF Zone (Q1428532)', 'Korean DMZ (south) (Q331990)', 'Korean DMZ (north) (Q331990)', 'USNB Guantanamo Bay (Q762570)', 'Cyprus U.N. Buffer Zone (Q116970)', 'Siachen Glacier (Q333946)', 'Akrotiri (Q9143535)', 'Paracel Is. (Q274388)', 'Coral Sea Is. (Q172216)', 'Spratly Is. (Q215664)', 'Clipperton I. (Q161258)', 'Bajo Nuevo Bank (Q1257783)', 'Serranilla Bank (Q1169008)', 'Scarborough Reef (Q628716)']\n",
      "Prop-only: Abkhazia (Q23334)\n",
      "Prop-only: Kingdom of Denmark (Q756617)\n",
      "Prop-only: Kingdom of the Netherlands (Q29999)\n",
      "Prop-only: South Ossetia (Q23427)\n",
      "Prop-only: United Kingdom (Q145)\n",
      "280 QIDs in common between prop-values and geometries.\n"
     ]
    }
   ],
   "source": [
    "# load in data\n",
    "# I skip a few regions that have coordinates -- see https://github.com/geohci/wiki-region-groundtruth/blob/main/resources/REGIONS.md\n",
    "# And a few regions are only present as Wikidata properties and don't have coordinates\n",
    "# Abkhazia / South Ossetia just aren't in the geographic data\n",
    "# the Kingdoms are agglomerations of regions but if I didn't include them, certain\n",
    "# Wikidata items that e.g., link to UK but not England specifically would be missed\n",
    "region_properties = get_region_properties(os.path.join(data_dir, properties_tsv[0]))\n",
    "region_shapes, qid_to_region = get_region_data(os.path.join(data_dir, region_tsv[0]),\n",
    "                                               os.path.join(data_dir, region_geoms[0]),\n",
    "                                               os.path.join(data_dir, aggregation_tsv[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|qid   |region        |\n",
      "+------+--------------+\n",
      "|Q23334|Abkhazia      |\n",
      "|Q889  |Afghanistan   |\n",
      "|Q5689 |Ã…land Islands |\n",
      "|Q222  |Albania       |\n",
      "|Q262  |Algeria       |\n",
      "|Q16641|American Samoa|\n",
      "|Q228  |Andorra       |\n",
      "|Q916  |Angola        |\n",
      "|Q25228|Anguilla      |\n",
      "|Q51   |Antarctica    |\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create table with QID -> Region mapping for making data in end result more readable\n",
    "spark.createDataFrame(pd.DataFrame(qid_to_region.items(), columns=['qid', 'region'])).createOrReplaceTempView('qid_to_region')\n",
    "spark.sql(\"SELECT * FROM qid_to_region LIMIT 10\").show(50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.pointInCountry(lon, lat)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pointInCountry(lon, lat):\n",
    "    \"\"\"Determine which region contains a lat-lon coordinate.\n",
    "    \n",
    "    Depends on shapely library and region_shapes object, which contains a dictionary\n",
    "    mapping QIDs to shapely geometry objects.\n",
    "    \"\"\"\n",
    "    pt = Point(lon, lat)\n",
    "    for qid in region_shapes:\n",
    "        if region_shapes[qid].contains(pt):\n",
    "            return qid\n",
    "    return \"N/A\"\n",
    "    \n",
    "spark.udf.register('pointInCountry', pointInCountry, 'String')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getLon(obj)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value info in wikidata entity table (https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Wikidata_entity)\n",
    "# is a string as opposed to struct (because it has a variable schema)\n",
    "# this UDF extracts the QID value (or null if doesn't exist)\n",
    "def getWikidataValue(obj):\n",
    "    try:\n",
    "        d = json.loads(obj)\n",
    "        return d.get('id')\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getWikidataValue', getWikidataValue, 'String')\n",
    "\n",
    "# specific functions for getting lat and lon out of the P625 property in the Wikidata entity\n",
    "def getLat(obj):\n",
    "    try:\n",
    "        d = json.loads(obj)\n",
    "        return d.get('latitude')\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def getLon(obj):\n",
    "    try:\n",
    "        d = json.loads(obj)\n",
    "        return d.get('longitude')\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getLat', getLat, 'Float')\n",
    "spark.udf.register('getLon', getLon, 'Float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 41.0760632218953,\n",
       " 'longitude': 0.4121379044185,\n",
       " 'altitude': None,\n",
       " 'precision': 1e-06,\n",
       " 'globe': 'http://www.wikidata.org/entity/Q2'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads('{\"latitude\":41.0760632218953,\"longitude\":0.4121379044185,\"altitude\":null,\"precision\":1.0E-6,\"globe\":\"http://www.wikidata.org/entity/Q2\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P19', 'P17', 'P27', 'P495', 'P131', 'P1532', 'P3842', 'P361', 'P1269')\n"
     ]
    }
   ],
   "source": [
    "dump_snapshot = '2020-11'\n",
    "wd_snapshot = '2020-12-07'\n",
    "prop_list = tuple([p[0] for p in region_properties])\n",
    "print(prop_list)\n",
    "tablename = 'isaacj.qid_to_country_2020_12_07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CREATE TABLE IF NOT EXISTS isaacj.qid_to_country_2020_12_07 (\n",
      "        qid              STRING  COMMENT 'Wikidata ID of item with at least one Wikipedia sitelink -- e.g., Q42',\n",
      "        property         STRING  COMMENT 'Wikidata property (e.g., P625 for coordinates) from which country was derived',\n",
      "        country          STRING  COMMENT 'Region name'\n",
      "    )\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "do_execute = True\n",
    "create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {0} (\n",
    "        qid              STRING  COMMENT 'Wikidata ID of item with at least one Wikipedia sitelink -- e.g., Q42',\n",
    "        property         STRING  COMMENT 'Wikidata property (e.g., P625 for coordinates) from which country was derived',\n",
    "        country          STRING  COMMENT 'Region name'\n",
    "    )\n",
    "    \"\"\".format(tablename)\n",
    "\n",
    "if do_execute:\n",
    "    print(create_table_query)\n",
    "    spark.sql(create_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH relevant_wikis AS (\n",
      "    SELECT\n",
      "      DISTINCT(dbname) AS wiki_db\n",
      "    FROM wmf_raw.mediawiki_project_namespace_map\n",
      "    WHERE\n",
      "      snapshot = '2020-11'\n",
      "      AND hostname = 'ha.wikipedia.org'\n",
      "),\n",
      "relevant_qids AS (\n",
      "    SELECT\n",
      "      DISTINCT(item_id) AS item_id\n",
      "    FROM wmf.wikidata_item_page_link wd\n",
      "    INNER JOIN relevant_wikis db\n",
      "      ON (wd.wiki_db = db.wiki_db)\n",
      "    WHERE\n",
      "      snapshot = '2020-12-07'\n",
      "      AND page_namespace = 0\n",
      "),\n",
      "exploded_statements AS (\n",
      "    SELECT\n",
      "      id AS item_id,\n",
      "      explode(claims) AS claim\n",
      "    FROM wmf.wikidata_entity w\n",
      "    INNER JOIN relevant_qids q\n",
      "      ON (w.id = q.item_id)\n",
      "    WHERE\n",
      "      w.snapshot = '2020-12-07'\n",
      "),\n",
      "lat_lon_coords AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      getLat(claim.mainSnak.dataValue.value) as lat,\n",
      "      getLon(claim.mainSnak.dataValue.value) as lon\n",
      "    FROM exploded_statements\n",
      "    WHERE\n",
      "      claim.mainSnak.property = 'P625'\n",
      "),\n",
      "geolocated AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      pointInCountry(lon, lat) AS country_qid\n",
      "    FROM lat_lon_coords\n",
      "),\n",
      "coordinate_countries AS (\n",
      "    SELECT\n",
      "      item_id AS item_id,\n",
      "      q.region AS country\n",
      "    FROM geolocated g\n",
      "    INNER JOIN qid_to_region q\n",
      "      ON (g.country_qid = q.qid)\n",
      "),\n",
      "relevant_statements AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      claim.mainSnak.property AS property,\n",
      "      getWikidataValue(claim.mainSnak.dataValue.value) as value\n",
      "    FROM exploded_statements\n",
      "    WHERE\n",
      "      claim.mainSnak.property IN ('P19', 'P17', 'P27', 'P495', 'P131', 'P1532', 'P3842', 'P361', 'P1269')\n",
      "),\n",
      "property_countries AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      property,\n",
      "      q.region AS country\n",
      "    FROM relevant_statements r\n",
      "    INNER JOIN qid_to_region q\n",
      "      ON (r.value = q.qid)\n",
      ")\n",
      "INSERT OVERWRITE TABLE isaacj.qid_to_country_2020_12_07\n",
      "SELECT\n",
      "  item_id,\n",
      "  'P625',\n",
      "  country\n",
      "FROM coordinate_countries\n",
      "UNION ALL\n",
      "SELECT\n",
      "  item_id,\n",
      "  property,\n",
      "  country\n",
      "FROM property_countries\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o62.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:195)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:99)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 64 in stage 21.0 failed 4 times, most recent failure: Lost task 64.3 in stage 21.0 (TID 8946, an-worker1115.eqiad.wmnet, executor 508): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 214, in geos_point_from_py\n    dx = c_double(coords[0])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 28 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 214, in geos_point_from_py\n    dx = c_double(coords[0])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d11eb0b465fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:195)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:99)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 64 in stage 21.0 failed 4 times, most recent failure: Lost task 64.3 in stage 21.0 (TID 8946, an-worker1115.eqiad.wmnet, executor 508): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 214, in geos_point_from_py\n    dx = c_double(coords[0])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 28 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/c/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000570/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 214, in geos_point_from_py\n    dx = c_double(coords[0])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Desired full query is below that would grab country data from a set of ~10 properties\n",
    "AND grab country data based on an item's coordinates\n",
    "However...this query failed for all wikis, simiplewiki only, and even hausawiki (tiny)\n",
    "so some work needs to be done.\n",
    "The error message isn't particularly useful -- just says a worker failed the max number of tries.\n",
    "\n",
    "I then split this query into its two component parts (property matching and coordinate geolocating)\n",
    "and the former (property matching) works, which makes sense because it's quite simple.\n",
    "The latter (coordinate geolocation) by itself triggers the error, so that's where the problem seems to be\n",
    "\n",
    "Explanation of CTEs:\n",
    "* relevant_wikis: get list of Wikipedia wiki_dbs (e.g., enwiki) so as to limit the\n",
    "    the Wikidata items considered to just those with Wikipedia sitelinks\n",
    "* relevant_qids: get set of Wikidata item IDs that have at least one Wikipedia sitelink\n",
    "* exploded_statements: explode Wikidata entity data to one Wikidata claim per row\n",
    "* lat_lon_coords: extract lat/lon values from claims to be geolocated\n",
    "* geolocated: pass lat/lon values to UDF to identify which country they are in.\n",
    "    This I believe is where all the heavy lifting is occurring...\n",
    "* coordinate_countries: map country QIDs for geolocation to country names\n",
    "* relevant_statements: get all Wikidata claims that might have a country value\n",
    "* property_countries: extract any country values from these claims\n",
    "* INSERT: union together coordinate countries and property countries into single table\n",
    "\"\"\"\n",
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH relevant_wikis AS (\n",
    "    SELECT\n",
    "      DISTINCT(dbname) AS wiki_db\n",
    "    FROM wmf_raw.mediawiki_project_namespace_map\n",
    "    WHERE\n",
    "      snapshot = '{0}'\n",
    "      AND hostname LIKE '%.wikipedia.org'\n",
    "),\n",
    "relevant_qids AS (\n",
    "    SELECT\n",
    "      DISTINCT(item_id) AS item_id\n",
    "    FROM wmf.wikidata_item_page_link wd\n",
    "    INNER JOIN relevant_wikis db\n",
    "      ON (wd.wiki_db = db.wiki_db)\n",
    "    WHERE\n",
    "      snapshot = '{1}'\n",
    "      AND page_namespace = 0\n",
    "),\n",
    "exploded_statements AS (\n",
    "    SELECT\n",
    "      id AS item_id,\n",
    "      explode(claims) AS claim\n",
    "    FROM wmf.wikidata_entity w\n",
    "    INNER JOIN relevant_qids q\n",
    "      ON (w.id = q.item_id)\n",
    "    WHERE\n",
    "      w.snapshot = '{1}'\n",
    "),\n",
    "lat_lon_coords AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      getLat(claim.mainSnak.dataValue.value) as lat,\n",
    "      getLon(claim.mainSnak.dataValue.value) as lon\n",
    "    FROM exploded_statements\n",
    "    WHERE\n",
    "      claim.mainSnak.property = 'P625'\n",
    "),\n",
    "geolocated AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      pointInCountry(lon, lat) AS country_qid\n",
    "    FROM lat_lon_coords\n",
    "),\n",
    "coordinate_countries AS (\n",
    "    SELECT\n",
    "      item_id AS item_id,\n",
    "      q.region AS country\n",
    "    FROM geolocated g\n",
    "    INNER JOIN qid_to_region q\n",
    "      ON (g.country_qid = q.qid)\n",
    "),\n",
    "relevant_statements AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      claim.mainSnak.property AS property,\n",
    "      getWikidataValue(claim.mainSnak.dataValue.value) as value\n",
    "    FROM exploded_statements\n",
    "    WHERE\n",
    "      claim.mainSnak.property IN {2}\n",
    "),\n",
    "property_countries AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      property,\n",
    "      q.region AS country\n",
    "    FROM relevant_statements r\n",
    "    INNER JOIN qid_to_region q\n",
    "      ON (r.value = q.qid)\n",
    ")\n",
    "INSERT OVERWRITE TABLE {3}\n",
    "SELECT\n",
    "  item_id,\n",
    "  'P625',\n",
    "  country\n",
    "FROM coordinate_countries\n",
    "UNION ALL\n",
    "SELECT\n",
    "  item_id,\n",
    "  property,\n",
    "  country\n",
    "FROM property_countries\n",
    "\"\"\".format(dump_snapshot, wd_snapshot, prop_list, tablename)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH relevant_wikis AS (\n",
      "    SELECT\n",
      "      DISTINCT(dbname) AS wiki_db\n",
      "    FROM wmf_raw.mediawiki_project_namespace_map\n",
      "    WHERE\n",
      "      snapshot = '2020-11'\n",
      "      AND hostname = 'ha.wikipedia.org'\n",
      "),\n",
      "relevant_qids AS (\n",
      "    SELECT\n",
      "      DISTINCT(item_id) AS item_id\n",
      "    FROM wmf.wikidata_item_page_link wd\n",
      "    INNER JOIN relevant_wikis db\n",
      "      ON (wd.wiki_db = db.wiki_db)\n",
      "    WHERE\n",
      "      snapshot = '2020-12-07'\n",
      "      AND page_namespace = 0\n",
      "),\n",
      "exploded_statements AS (\n",
      "    SELECT\n",
      "      id AS item_id,\n",
      "      explode(claims) AS claim\n",
      "    FROM wmf.wikidata_entity w\n",
      "    INNER JOIN relevant_qids q\n",
      "      ON (w.id = q.item_id)\n",
      "    WHERE\n",
      "      w.snapshot = '2020-12-07'\n",
      "),\n",
      "lat_lon_coords AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      getLat(claim.mainSnak.dataValue.value) as lat,\n",
      "      getLon(claim.mainSnak.dataValue.value) as lon\n",
      "    FROM exploded_statements\n",
      "    WHERE\n",
      "      claim.mainSnak.property = 'P625'\n",
      "),\n",
      "relevant_statements AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      claim.mainSnak.property AS property,\n",
      "      getWikidataValue(claim.mainSnak.dataValue.value) as value\n",
      "    FROM exploded_statements\n",
      "    WHERE\n",
      "      claim.mainSnak.property IN ('P19', 'P17', 'P27', 'P495', 'P131', 'P1532', 'P3842', 'P361', 'P1269')\n",
      "),\n",
      "property_countries AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      property,\n",
      "      q.region AS country\n",
      "    FROM relevant_statements r\n",
      "    INNER JOIN qid_to_region q\n",
      "      ON (r.value = q.qid)\n",
      ")\n",
      "INSERT OVERWRITE TABLE isaacj.qid_to_country_2020_12_07\n",
      "SELECT\n",
      "  item_id,\n",
      "  property,\n",
      "  country\n",
      "FROM property_countries\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Properties only -- works just fine for Hausa Wikipedia (and I suspect all wikis)\n",
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH relevant_wikis AS (\n",
    "    SELECT\n",
    "      DISTINCT(dbname) AS wiki_db\n",
    "    FROM wmf_raw.mediawiki_project_namespace_map\n",
    "    WHERE\n",
    "      snapshot = '{0}'\n",
    "      AND hostname = 'ha.wikipedia.org'\n",
    "),\n",
    "relevant_qids AS (\n",
    "    SELECT\n",
    "      DISTINCT(item_id) AS item_id\n",
    "    FROM wmf.wikidata_item_page_link wd\n",
    "    INNER JOIN relevant_wikis db\n",
    "      ON (wd.wiki_db = db.wiki_db)\n",
    "    WHERE\n",
    "      snapshot = '{1}'\n",
    "      AND page_namespace = 0\n",
    "),\n",
    "exploded_statements AS (\n",
    "    SELECT\n",
    "      id AS item_id,\n",
    "      explode(claims) AS claim\n",
    "    FROM wmf.wikidata_entity w\n",
    "    INNER JOIN relevant_qids q\n",
    "      ON (w.id = q.item_id)\n",
    "    WHERE\n",
    "      w.snapshot = '{1}'\n",
    "),\n",
    "lat_lon_coords AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      getLat(claim.mainSnak.dataValue.value) as lat,\n",
    "      getLon(claim.mainSnak.dataValue.value) as lon\n",
    "    FROM exploded_statements\n",
    "    WHERE\n",
    "      claim.mainSnak.property = 'P625'\n",
    "),\n",
    "relevant_statements AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      claim.mainSnak.property AS property,\n",
    "      getWikidataValue(claim.mainSnak.dataValue.value) as value\n",
    "    FROM exploded_statements\n",
    "    WHERE\n",
    "      claim.mainSnak.property IN {2}\n",
    "),\n",
    "property_countries AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      property,\n",
    "      q.region AS country\n",
    "    FROM relevant_statements r\n",
    "    INNER JOIN qid_to_region q\n",
    "      ON (r.value = q.qid)\n",
    ")\n",
    "INSERT OVERWRITE TABLE {3}\n",
    "SELECT\n",
    "  item_id,\n",
    "  property,\n",
    "  country\n",
    "FROM property_countries\n",
    "\"\"\".format(dump_snapshot, wd_snapshot, prop_list, tablename)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH relevant_wikis AS (\n",
      "    SELECT\n",
      "      DISTINCT(dbname) AS wiki_db\n",
      "    FROM wmf_raw.mediawiki_project_namespace_map\n",
      "    WHERE\n",
      "      snapshot = '2020-11'\n",
      "      AND hostname = 'ha.wikipedia.org'\n",
      "),\n",
      "relevant_qids AS (\n",
      "    SELECT\n",
      "      DISTINCT(item_id) AS item_id\n",
      "    FROM wmf.wikidata_item_page_link wd\n",
      "    INNER JOIN relevant_wikis db\n",
      "      ON (wd.wiki_db = db.wiki_db)\n",
      "    WHERE\n",
      "      snapshot = '2020-12-07'\n",
      "      AND page_namespace = 0\n",
      "),\n",
      "exploded_statements AS (\n",
      "    SELECT\n",
      "      id AS item_id,\n",
      "      explode(claims) AS claim\n",
      "    FROM wmf.wikidata_entity w\n",
      "    INNER JOIN relevant_qids q\n",
      "      ON (w.id = q.item_id)\n",
      "    WHERE\n",
      "      w.snapshot = '2020-12-07'\n",
      "),\n",
      "lat_lon_coords AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      getLat(claim.mainSnak.dataValue.value) as lat,\n",
      "      getLon(claim.mainSnak.dataValue.value) as lon\n",
      "    FROM exploded_statements\n",
      "    WHERE\n",
      "      claim.mainSnak.property = 'P625'\n",
      "),\n",
      "geolocated AS (\n",
      "    SELECT\n",
      "      item_id,\n",
      "      pointInCountry(lon, lat) AS country_qid\n",
      "    FROM lat_lon_coords\n",
      "),\n",
      "coordinate_countries AS (\n",
      "    SELECT\n",
      "      item_id AS item_id,\n",
      "      q.region AS country\n",
      "    FROM geolocated g\n",
      "    INNER JOIN qid_to_region q\n",
      "      ON (g.country_qid = q.qid)\n",
      ")\n",
      "INSERT OVERWRITE TABLE isaacj.qid_to_country_2020_12_07\n",
      "SELECT\n",
      "  item_id AS item_id,\n",
      "  'P625' AS property,\n",
      "  country AS country\n",
      "FROM coordinate_countries\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o62.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:195)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:99)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 184 in stage 34.0 failed 4 times, most recent failure: Lost task 184.3 in stage 34.0 (TID 13853, an-worker1112.eqiad.wmnet, executor 784): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 215, in geos_point_from_py\n    dy = c_double(coords[1])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 28 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 215, in geos_point_from_py\n    dy = c_double(coords[1])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2dd8e953756c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark2/python/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.sql.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.hive.execution.SaveAsHiveFile$class.saveAsHiveFile(SaveAsHiveFile.scala:86)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.saveAsHiveFile(InsertIntoHiveTable.scala:66)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.processInsert(InsertIntoHiveTable.scala:195)\n\tat org.apache.spark.sql.hive.execution.InsertIntoHiveTable.run(InsertIntoHiveTable.scala:99)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:115)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 184 in stage 34.0 failed 4 times, most recent failure: Lost task 184.3 in stage 34.0 (TID 13853, an-worker1112.eqiad.wmnet, executor 784): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 215, in geos_point_from_py\n    dy = c_double(coords[1])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 28 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-13-8f01e34c45b3>\", line 3, in pointInCountry\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 48, in __init__\n    self._set_coords(*args)\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 137, in _set_coords\n    self._geom, self._ndim = geos_point_from_py(tuple(args))\n  File \"/var/lib/hadoop/data/g/yarn/local/usercache/isaacj/appcache/application_1607972933093_50670/container_e33_1607972933093_50670_01_000868/venv/lib/python3.7/site-packages/shapely/geometry/point.py\", line 215, in geos_point_from_py\n    dy = c_double(coords[1])\nTypeError: must be real number, not NoneType\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage11.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1124)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1130)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n"
     ]
    }
   ],
   "source": [
    "# Coordinate geolocation only -- fails even for tiny Hausa Wikipedia\n",
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH relevant_wikis AS (\n",
    "    SELECT\n",
    "      DISTINCT(dbname) AS wiki_db\n",
    "    FROM wmf_raw.mediawiki_project_namespace_map\n",
    "    WHERE\n",
    "      snapshot = '{0}'\n",
    "      AND hostname = 'ha.wikipedia.org'\n",
    "),\n",
    "relevant_qids AS (\n",
    "    SELECT\n",
    "      DISTINCT(item_id) AS item_id\n",
    "    FROM wmf.wikidata_item_page_link wd\n",
    "    INNER JOIN relevant_wikis db\n",
    "      ON (wd.wiki_db = db.wiki_db)\n",
    "    WHERE\n",
    "      snapshot = '{1}'\n",
    "      AND page_namespace = 0\n",
    "),\n",
    "exploded_statements AS (\n",
    "    SELECT\n",
    "      id AS item_id,\n",
    "      explode(claims) AS claim\n",
    "    FROM wmf.wikidata_entity w\n",
    "    INNER JOIN relevant_qids q\n",
    "      ON (w.id = q.item_id)\n",
    "    WHERE\n",
    "      w.snapshot = '{1}'\n",
    "),\n",
    "lat_lon_coords AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      getLat(claim.mainSnak.dataValue.value) as lat,\n",
    "      getLon(claim.mainSnak.dataValue.value) as lon\n",
    "    FROM exploded_statements\n",
    "    WHERE\n",
    "      claim.mainSnak.property = 'P625'\n",
    "),\n",
    "geolocated AS (\n",
    "    SELECT\n",
    "      item_id,\n",
    "      pointInCountry(lon, lat) AS country_qid\n",
    "    FROM lat_lon_coords\n",
    "),\n",
    "coordinate_countries AS (\n",
    "    SELECT\n",
    "      item_id AS item_id,\n",
    "      q.region AS country\n",
    "    FROM geolocated g\n",
    "    INNER JOIN qid_to_region q\n",
    "      ON (g.country_qid = q.qid)\n",
    ")\n",
    "INSERT OVERWRITE TABLE {3}\n",
    "SELECT\n",
    "  item_id AS item_id,\n",
    "  'P625' AS property,\n",
    "  country AS country\n",
    "FROM coordinate_countries\n",
    "\"\"\".format(dump_snapshot, wd_snapshot, prop_list, tablename)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
